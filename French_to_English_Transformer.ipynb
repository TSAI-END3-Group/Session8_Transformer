{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set taken is from a previous tutorial on Transation from French to English  which is available at https://download.pytorch.org/tutorial/data.zip\n",
    "We are trying to use the same data and see how the transformer gives the output compared to the one that was available using the Seq2Seq network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.legacy.data import Field,TabularDataset,BucketIterator  \n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import Multi30k\n",
    "from typing import Iterable, List\n",
    "from spacy.lang.fr.examples import sentences \n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### this is the step to convert a single file to the training and testing. \n",
    "## then convert it to the form that it is being accepted by the torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      Go.  \\\n",
      "0                                                    Run!   \n",
      "1                                                    Run!   \n",
      "2                                                    Wow!   \n",
      "3                                                   Fire!   \n",
      "4                                                   Help!   \n",
      "...                                                   ...   \n",
      "135836  A carbon footprint is the amount of carbon dio...   \n",
      "135837  Death is something that we're often discourage...   \n",
      "135838  Since there are usually multiple websites on a...   \n",
      "135839  If someone who doesn't know your background sa...   \n",
      "135840  It may be impossible to get a completely error...   \n",
      "\n",
      "                                                     Va !  \n",
      "0                                                 Cours !  \n",
      "1                                                Courez !  \n",
      "2                                              Ça alors !  \n",
      "3                                                Au feu !  \n",
      "4                                              À l'aide !  \n",
      "...                                                   ...  \n",
      "135836  Une empreinte carbone est la somme de pollutio...  \n",
      "135837  La mort est une chose qu'on nous décourage sou...  \n",
      "135838  Puisqu'il y a de multiples sites web sur chaqu...  \n",
      "135839  Si quelqu'un qui ne connaît pas vos antécédent...  \n",
      "135840  Il est peut-être impossible d'obtenir un Corpu...  \n",
      "\n",
      "[135841 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('data/eng-fra.txt',sep=\"\\t\")\n",
    "df=df.applymap(lambda x: x.replace('\"', ''))\n",
    "print(df)\n",
    "df.columns=['TRG','SRC']\n",
    "df.head()\n",
    "\n",
    "# # Creating a dataframe with 75%\n",
    "# # values of original dataframe\n",
    "train_data = df.sample(frac = 0.75)\n",
    " \n",
    "# Creating dataframe with\n",
    "# rest of the 25% values\n",
    "test_data = df.drop(train_data.index)\n",
    "\n",
    "train_data.to_csv(\"data/single_train_data.csv\")\n",
    "test_data.to_csv(\"data/single_test_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_fr = spacy.load('fr_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fr(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_fr.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(tokenize = tokenize_fr, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True, \n",
    "            batch_first = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True, \n",
    "            batch_first = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields={'SRC':('SRC',SRC),'TRG':('TRG',TRG)}\n",
    "\n",
    "train_data, test_data=TabularDataset.splits(\n",
    "                                    path='data',\n",
    "                                    train='single_train_data.csv',#this is the traing file\n",
    "                                    test='single_test_data.csv',##this is the test file\n",
    "                                    format='csv',\n",
    "                                    fields=fields)## need to put zero if only one data is being returned https://github.com/pytorch/text/issues/474\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 1)\n",
    "TRG.build_vocab(train_data, min_freq = 1)##the vocabulary will be out of the trainig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12096"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TRG.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainingData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, test_data), \n",
    "     batch_size = 64,sort=False,\n",
    "     device = device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2,    5,   13,  ...,    1,    1,    1],\n",
      "        [   2,   69,   35,  ...,    1,    1,    1],\n",
      "        [   2,   20, 5797,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [   2,    5,   16,  ...,    1,    1,    1],\n",
      "        [   2,    5,   37,  ...,    1,    1,    1],\n",
      "        [   2,   29,    9,  ...,    1,    1,    1]], device='cuda:0')\n",
      "tensor([[  2,   5, 254,  ...,   1,   1,   1],\n",
      "        [  2,  19,   6,  ...,   1,   1,   1],\n",
      "        [  2,   5,  35,  ...,   1,   1,   1],\n",
      "        ...,\n",
      "        [  2,   5,  53,  ...,   1,   1,   1],\n",
      "        [  2,   5,  35,  ...,   1,   1,   1],\n",
      "        [  2,  13,  21,  ...,   1,   1,   1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i,batch in enumerate(train_iterator):\n",
    "    print(batch.SRC)\n",
    "    print(batch.TRG)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['on',\n",
       " 'ne',\n",
       " 'peut',\n",
       " 'pas',\n",
       " 'être',\n",
       " 'à',\n",
       " 'deux',\n",
       " 'endroits',\n",
       " 'au',\n",
       " 'même',\n",
       " 'moment',\n",
       " '.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(train_data.examples[1])['SRC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### now we will start building the encoder class\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim,#length of the vocabulary\n",
    "                 hid_dim,#512\n",
    "                 n_layers,#6\n",
    "                 n_heads,#8\n",
    "                 pf_dim,#output dimention\n",
    "                 dropout,\n",
    "                 device,#cuda\n",
    "                 max_length=100):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device=device\n",
    "        \n",
    "        #input embedding\n",
    "        self.tok_embedding=nn.Embedding(input_dim,hid_dim)\n",
    "        ## convert the indices of the words to the position embedding. since there can be just \n",
    "        ## 100 tokens in the sentence hence the embedding will be a matrix of 100*hid_dim\n",
    "        self.pos_embedding=nn.Embedding(max_length,hid_dim) \n",
    "        \n",
    "        ##create layers ..what happens in each layer is listed\n",
    "        self.layers=nn.ModuleList([EncoderLayer(hid_dim, ## this is 512\n",
    "                                                n_heads, ## MHA has n_heads\n",
    "                                                pf_dim, ## the dimentions of output\n",
    "                                                dropout,\n",
    "                                                device\n",
    "                                                )for _ in range(n_layers)])\n",
    "        \n",
    "        \n",
    "        \n",
    "        ##define dropout\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.scale=torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    \n",
    "    ##src=it is a set of input values that are in the form of batch\n",
    "    ##after the Encoder running will get the inputs for the decoder\n",
    "    def forward(self,src,src_mask):\n",
    "        batch_size=src.shape[0]\n",
    "        src_len=src.shape[1]\n",
    "        \n",
    "        ##it will fill the pos vector with the src_len numbers which are repeated batch_size\n",
    "        ## also for the combination to happen between the pos vector and the input they \n",
    "        ## must be of the same dimention\n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        \n",
    "        ##this is how the batch will look like after combining the scaling factor, positional embedding\n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        \n",
    "        \n",
    "        ##pass it to all the layers...these layers is the MHA & feedforward network + residual connections\n",
    "        ##after the operations also the same variable src is being updated\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "            \n",
    "        return src\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hid_dim, \n",
    "                 n_heads, \n",
    "                 pf_dim,  \n",
    "                 dropout, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        # the two norm layers\n",
    "        self.self_attn_layer_norm=nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm=nn.LayerNorm(hid_dim)\n",
    "        \n",
    "        ## MultiHeadAttentionLayer is a class that we will call\n",
    "        self.self_attention=MultiHeadAttentionLayer(hid_dim,n_heads,dropout,device)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        ## the feedforward\n",
    "        ## PositionwiseFeedforwardLayer is a class that will be called\n",
    "        self.positionwise_feedforward=PositionwiseFeedforwardLayer(hid_dim,pf_dim,dropout)\n",
    "        \n",
    "    ##src is the batch wise data\n",
    "    def forward(self,src,src_mask):\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]...basically each token will be of hid_dim\n",
    "        ##[[.1,.1,.2,.3,.4,.4,...512 entries]\n",
    "        ##[.1,.1,.2,.3,.4,.4,...512 entries]] two tokens whose embedding is 512 length and are in the form of a batch\n",
    "        \n",
    "        \n",
    "        #self_attention...it will take the query,key,value matrix and then calculate the self attention. Also to use\n",
    "        ## the same code for the decoder by passing a src_mask\n",
    "        #_src,_=self.self_attention(query=src,key=src,value=src,mask=src_mask)\n",
    "        \n",
    "        _src,_=self.self_attention(src,src,src,src_mask)\n",
    "        \n",
    "        ##apply the dropout, residual and pass to norm\n",
    "        src=self.self_attn_layer_norm(src+self.dropout(_src))\n",
    "        \n",
    "        \n",
    "        _src=self.positionwise_feedforward(src)\n",
    "        \n",
    "        src=self.ff_layer_norm(src+self.dropout(_src))\n",
    "        \n",
    "        return src\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "    \n",
    "        assert hid_dim % n_heads == 0\n",
    "        \n",
    "        self.hid_dim=hid_dim\n",
    "        self.n_heads=n_heads\n",
    "        self.head_dim=hid_dim // n_heads\n",
    "        \n",
    "        ##this is as good as definng a Wq...when ever we will have a linear layer there will be a W associated\n",
    "        ## also instead of decalring it for the head_dim we are declaring it for the hidden dimention and then \n",
    "        ## we will divide the matrix into the n_heads.. this way the code is much general\n",
    "        self.fc_q=nn.Linear(hid_dim,hid_dim)\n",
    "        self.fc_k=nn.Linear(hid_dim,hid_dim)\n",
    "        self.fc_v=nn.Linear(hid_dim,hid_dim)\n",
    "        \n",
    "        ## this is the last weight matrix that will be used at the time of concatenation of the outputs        \n",
    "        self.fc_o=nn.Linear(hid_dim,hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        ##the scaling factor as in scale dot products...this will be the sqrt of the dimention that we are using\n",
    "        self.scale=torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        \n",
    "    \n",
    "    \n",
    "    ## there are lot of transformation happeing\n",
    "    ## [batch,word,hid_dim] -> [batch,word,n_head,head_dim] (view)-> [batch,n_head,word,head_dim] (permute)\n",
    "    ## ->[batch,words,number_heads,head_dim] (permute +contiguous) -> [batch,words,hid_dim] (view)\n",
    "    ##\n",
    "    ##\n",
    "    def forward(self,query,key,value,mask=None):\n",
    "        \n",
    "        #assuming that the matrices have an additional batch dimention\n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        #Q=[batch,word,hid_dim]\n",
    "        Q=self.fc_q(query)\n",
    "        K=self.fc_k(key)\n",
    "        V=self.fc_v(value)\n",
    "        \n",
    "        #Q is of the dimention [batch,word,hid_dim]\n",
    "        # we want to create new Q which is [batch,num_head,words,head_dim]\n",
    "        # it is a 2 step thing...\n",
    "        # step1 [batch,word,hid_dim] -> [batch,word,n_head,head_dim] using views\n",
    "        #step2 using permute change the axis from [batch,word,n_head,head_dim] -> [batch,n_head,word,head_dim]\n",
    "        Q=Q.view(batch_size,-1,self.n_heads,self.head_dim).permute(0,2,1,3)\n",
    "        K=K.view(batch_size,-1,self.n_heads,self.head_dim).permute(0,2,1,3)\n",
    "        V=V.view(batch_size,-1,self.n_heads,self.head_dim).permute(0,2,1,3)\n",
    "        \n",
    "        \n",
    "        ## now compuet the energy ..we need to take a transpose of K\n",
    "        ## this will give us energy dimentions as [batch_len,number_heads,number_words,number_words]\n",
    "        energy=torch.matmul(Q,K.permute(0,1,3,2))/self.scale\n",
    "        #0, 1, 3, 2\n",
    "        ## this is how we will use the mask\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        #attentiox=[batch,number_heads,words,words]\n",
    "        attention=torch.softmax(energy,dim=-1)\n",
    "        \n",
    "        ##this will give x=[batch,number_heads,words,head_dim]\n",
    "        x=torch.matmul(self.dropout(attention),V)\n",
    "        \n",
    "        #batchnumber,words,heads,head_dim..this is some memory optimization operation\n",
    "        # but the x is also changed from [batch,number_heads,words,head_dim] -> [batch,words,number_heads,head_dim]\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        \n",
    "        #now number_heads and head_dim are getting combined together to get the single continuous tensor\n",
    "        #[batch,words,number_heads,head_dim] -> [batch,words,hid_dim]\n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        \n",
    "        \n",
    "        ##pass it through the linear layer\n",
    "        x=self.fc_o(x)\n",
    "        \n",
    "        \n",
    "        #x=x = [batch size, words, hid dim]\n",
    "        return x,attention\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "##positionwise feed forward layer\n",
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #x = [batch size, seq len, hid_dim]->[batch size, seq len, pf_dim]    \n",
    "        x=self.dropout(torch.relu(self.fc_1(x)))\n",
    "        \n",
    "        #x=[batch size, seq len, pf_dim]->[batch size, seq len, hid_dim]\n",
    "        x=self.fc_2(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "##decoder\n",
    "##this should be simialr to the encoder class so I am strating by copying the encoder class\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 output_dim,#length of the vocabulary\n",
    "                 hid_dim,#512\n",
    "                 n_layers,#6\n",
    "                 n_heads,#8\n",
    "                 pf_dim,#output dimention\n",
    "                 dropout,\n",
    "                 device,#cuda\n",
    "                 max_length=100):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device=device\n",
    "        \n",
    "        #input embedding\n",
    "        self.tok_embedding=nn.Embedding(output_dim,hid_dim)\n",
    "        ## convert the indices of the words to the position embedding. since there can be just \n",
    "        ## 100 tokens in the sentence hence the embedding will be a matrix of 100*hid_dim\n",
    "        self.pos_embedding=nn.Embedding(max_length,hid_dim) \n",
    "        \n",
    "        ##create layers ..what happens in each layer is listed\n",
    "        self.layers=nn.ModuleList([DecoderLayer(hid_dim, ## this is 512\n",
    "                                                n_heads, ## MHA has n_heads\n",
    "                                                pf_dim, ## the dimentions of output\n",
    "                                                dropout,\n",
    "                                                device\n",
    "                                                )\n",
    "                                    for _ in range(n_layers)])\n",
    "        \n",
    "        \n",
    "        ##thiis is extra\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        ##define dropout\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.scale=torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    \n",
    "    ## the forward function will be little different than the Encoder\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        batch_size=trg.shape[0]\n",
    "        trg_len=trg.shape[1]\n",
    "        \n",
    "        ##it will fill the pos vector with the src_len numbers which are repeated batch_size\n",
    "        ## also for the combination to happen between the pos vector and the input they \n",
    "        ## must be of the same dimention\n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        \n",
    "        ##this is how the batch will look like after combining the scaling factor, positional embedding\n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "        \n",
    "        \n",
    "        ##pass it to all the layers...these layers is the MHA & feedforward network + residual connections\n",
    "        ##after the operations also the same variable src is being updated\n",
    "        for layer in self.layers:\n",
    "            trg,attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "            \n",
    "        output=self.fc_out(trg)\n",
    "        return output,attention\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hid_dim, \n",
    "                 n_heads, \n",
    "                 pf_dim, \n",
    "                 dropout, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        #norm for the self attention\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        \n",
    "        #norm for the encoder _attentions\n",
    "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        \n",
    "        #the feedfoward layer normalization..this one is after the encoder-attentions\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        \n",
    "        ## calculate the self attentions\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        \n",
    "        # calculate the attention using the K,V coming from encoder\n",
    "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        \n",
    "        #feedforward\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
    "                                                                     pf_dim, \n",
    "                                                                     dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,trg,enc_src,trg_mask,src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #trg_mask = [batch size, trg len]\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "            \n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        \n",
    "        #encoder attention Q is coming from Decoder while K and V are coming from encoder\n",
    "        # the src_mask is to stop the decoder from using the <PAD> values in case if any\n",
    "        #enc_src=[batchsize,source_len,hid_dim]\n",
    "        #src_mask=[batbatch_size,source_len]\n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        \n",
    "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "                    \n",
    "        _trg=self.positionwise_feedforward(trg)\n",
    "        \n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        \n",
    "        return trg,attention\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "##this module will encapsulate the encoder -decoder piece . it will also take care of the maskings\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, \n",
    "                 encoder, \n",
    "                 decoder, \n",
    "                 src_pad_idx, \n",
    "                 trg_pad_idx, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    #this is just to add the 0s at the places which are pad\n",
    "    def make_src_mask(self, src):        \n",
    "        #src = [batch size, src len] \n",
    "        ## we are chaecking if the word is not a <PAD>..\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        return src_mask\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        #first check for the padding and make them 0\n",
    "        ##this is also checking if the word is not a PAD\n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "\n",
    "        #if the padding is giving some more locatiosn as 0 then include them as well\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        \n",
    "        return trg_mask\n",
    "         \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len]\n",
    "    def forward(self,src,trg):\n",
    "            \n",
    "        #src_mask=[batch size,1,1,src len]\n",
    "        src_mask = self.make_src_mask(src)\n",
    "\n",
    "        #trg_mask=[batch size,1,1,trg len]\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "\n",
    "        #enc_src=[batch size, src len, hid dim]\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "\n",
    "        ##output = [batch size, trg len, output dim]\n",
    "        #attention = [batch size, n_heads, trg len, src len]  ..in case of the encoder it is bs,nhead,src_len,src_len\n",
    "        ## but in the case of the decoder Q is coming from decoder(aka target) so trg_len and \n",
    "        ## K is coming from encoder so src_len\n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        return output,attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 64\n",
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "enc = Encoder(INPUT_DIM, \n",
    "              HID_DIM, \n",
    "              ENC_LAYERS, \n",
    "              ENC_HEADS, \n",
    "              ENC_PF_DIM, \n",
    "              ENC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, \n",
    "              HID_DIM, \n",
    "              DEC_LAYERS, \n",
    "              DEC_HEADS, \n",
    "              DEC_PF_DIM, \n",
    "              DEC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 15,461,952 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "##weight initialization\n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.apply(initialize_weights);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "#     train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "#     train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    \n",
    "    #for src, tgt in train_dataloader:\n",
    "    count=0\n",
    "    for i,batch in enumerate(train_iterator):\n",
    "        src = batch.SRC.to(device)\n",
    "        tgt = batch.TRG.to(device)\n",
    "        # print(src.shape)\n",
    "        # print(tgt.shape)\n",
    "        # print(tgt[:,:-1].shape)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(src, tgt[:,:-1]) #[:,:-1])\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        tgt = tgt[:,1:].contiguous().view(-1)\n",
    "\n",
    "        # output = output[1:].view(-1, output.shape[-1])\n",
    "        # tgt = tgt[1:].reshape(-1) #tgt[:,:-1][1:].reshape(-1)\n",
    "        loss = loss_fn(output, tgt)\n",
    "        loss.backward()\n",
    "        clip = 1\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "        count+=1\n",
    "\n",
    "    return losses / count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "#     val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "#     val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    count=0\n",
    "    for i,batch in enumerate(test_iterator):\n",
    "        src = batch.SRC.to(device)\n",
    "        tgt = batch.TRG.to(device)[:,:-1]\n",
    "\n",
    "        output, _ = model(src, tgt[:,:-1])\n",
    "            \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        tgt = tgt[:,1:].contiguous().view(-1)\n",
    "\n",
    "        # output = output[1:].view(-1, output.shape[-1])\n",
    "        # tgt = tgt[1:].reshape(-1)\n",
    "        loss = loss_fn(output, tgt)\n",
    "        losses += loss.item()\n",
    "        count+=1\n",
    "    return losses / count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 2.864, Train PPL:  17.536 | Val loss: 1.701, Val. PPL:   5.480 | Epoch time = 54.723s\n",
      "Epoch: 2, Train loss: 1.520, Train PPL:   4.573 | Val loss: 1.232, Val. PPL:   3.429 | Epoch time = 55.496s\n",
      "Epoch: 3, Train loss: 1.095, Train PPL:   2.989 | Val loss: 1.094, Val. PPL:   2.985 | Epoch time = 55.959s\n",
      "Epoch: 4, Train loss: 0.871, Train PPL:   2.389 | Val loss: 1.021, Val. PPL:   2.775 | Epoch time = 56.227s\n",
      "Epoch: 5, Train loss: 0.727, Train PPL:   2.070 | Val loss: 0.995, Val. PPL:   2.704 | Epoch time = 57.041s\n",
      "Epoch: 6, Train loss: 0.628, Train PPL:   1.874 | Val loss: 0.984, Val. PPL:   2.675 | Epoch time = 56.153s\n",
      "Epoch: 7, Train loss: 0.554, Train PPL:   1.741 | Val loss: 0.979, Val. PPL:   2.662 | Epoch time = 56.122s\n",
      "Epoch: 8, Train loss: 0.500, Train PPL:   1.648 | Val loss: 0.977, Val. PPL:   2.656 | Epoch time = 56.240s\n",
      "Epoch: 9, Train loss: 0.455, Train PPL:   1.576 | Val loss: 0.977, Val. PPL:   2.657 | Epoch time = 56.316s\n",
      "Epoch: 10, Train loss: 0.420, Train PPL:   1.522 | Val loss: 0.983, Val. PPL:   2.671 | Epoch time = 56.428s\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 10\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(model, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(model)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Train PPL: {math.exp(train_loss):7.3f} | Val loss: {val_loss:.3f}, Val. PPL: {math.exp(val_loss):7.3f} | \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n",
    "    \n",
    "    model.eval()\n",
    "        \n",
    "#     if isinstance(sentence, str):\n",
    "#         src_tensor = text_transform[SRC_LANGUAGE](sentence).unsqueeze(0).to(device)\n",
    "#     else:\n",
    "#         src_tensor = text_transform[TGT_LANGUAGE](sentence).unsqueeze(0).to(device)\n",
    "    \n",
    "#     src_mask = model.make_src_mask(src_tensor)\n",
    "    \n",
    "    \n",
    "    if isinstance(sentence, str):\n",
    "        nlp = spacy.load('fr_core_news_sm')\n",
    "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "        \n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "    \n",
    "    src_mask = model.make_src_mask(src_tensor)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src_tensor, src_mask)\n",
    "\n",
    "    #trg_indexes = [trg_field[init_token]]\n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "\n",
    "\n",
    "    for i in range(max_len):\n",
    "\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "        trg_mask = model.make_trg_mask(trg_tensor)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        pred_token = output.argmax(2)[:,-1].item()\n",
    "        \n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        #if pred_token == trg_field[eos_token]:\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "            \n",
    "    \n",
    "    #trg_tokens = [trg_field.vocab.get_itos()[i] for i in trg_indexes]\n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "    \n",
    "    return trg_tokens[1:], attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def display_attention(sentence, translation, attention, n_heads = 8, n_rows = 4, n_cols = 2):\n",
    "    \n",
    "    assert n_rows * n_cols == n_heads\n",
    "    \n",
    "    fig = plt.figure(figsize=(15,25))\n",
    "    \n",
    "    for i in range(n_heads):\n",
    "        \n",
    "        ax = fig.add_subplot(n_rows, n_cols, i+1)\n",
    "        \n",
    "        _attention = attention.squeeze(0)[i].cpu().detach().numpy()\n",
    "\n",
    "        cax = ax.matshow(_attention, cmap='bone')\n",
    "\n",
    "        ax.tick_params(labelsize=12)\n",
    "        ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \n",
    "                           rotation=45)\n",
    "        ax.set_yticklabels(['']+translation)\n",
    "\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src =  vrai ?\n",
      "trg = really ?\n",
      "predicted trg = is it true ? <eos>\n",
      "****************************************\n",
      "src =  nous gagnâmes .\n",
      "trg = we won .\n",
      "predicted trg = we 're cooperating . <eos>\n",
      "****************************************\n",
      "src =  sois calme !\n",
      "trg = be calm .\n",
      "predicted trg = be quiet . <eos>\n",
      "****************************************\n",
      "src =  soyez gentille !\n",
      "trg = be nice .\n",
      "predicted trg = be nice . <eos>\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "for example_idx in range(8,12):\n",
    "\n",
    "    src = vars(test_data.examples[example_idx])['SRC']\n",
    "    trg = vars(test_data.examples[example_idx])['TRG']\n",
    "\n",
    "    print(\"src = \",' '.join(src))\n",
    "    print(\"trg =\" ,' '.join(trg))\n",
    "\n",
    "    translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
    "    print(\"predicted trg =\" ,' '.join(translation))\n",
    "    print(\"*\"*40)\n",
    "    #print(f'predicted trg = {' '.join(translation)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the example we can see that the can not has been replaced by can't\n",
    "duel is not present imn the vocab and hence it has been replaced by <Unk>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src =  j' ai agrippé la corde aussi longtemps que j' ai pu , mais j' ai finalement dû la lâcher .\n",
      "trg = i held onto the rope for as long as i could , but i finally had to let go .\n",
      "predicted trg = i held onto the rope for as long as i could , but i finally could at go let go . <eos>\n",
      "****************************************\n",
      "src =  j' espère que vous allez tous bien .\n",
      "trg = i hope you are all well .\n",
      "predicted trg = i hope you 're all right . <eos>\n",
      "****************************************\n",
      "src =  la musique numérique devient de plus en plus populaire .\n",
      "trg = digital music is becoming more and more popular .\n",
      "predicted trg = digital music is becoming more popular and more popular . <eos>\n",
      "****************************************\n",
      "src =  il ne sait pas jouer de la guitare .\n",
      "trg = he can not play guitar .\n",
      "predicted trg = he ca n't play guitar . <eos>\n",
      "****************************************\n",
      "src =  ça a été une bonne journée .\n",
      "trg = it was a good day .\n",
      "predicted trg = it was a good day . <eos>\n",
      "****************************************\n",
      "src =  j' aime la façon dont vous me traitez .\n",
      "trg = i like the way you treat me .\n",
      "predicted trg = i like the way you treat me . <eos>\n",
      "****************************************\n",
      "src =  ils ont été victorieux .\n",
      "trg = they were victorious .\n",
      "predicted trg = they were victorious . <eos>\n",
      "****************************************\n",
      "src =  tom est confus et effrayé .\n",
      "trg = tom is confused and scared .\n",
      "predicted trg = tom is confused and afraid . <eos>\n",
      "****************************************\n",
      "src =  je suis allé en europe avant la guerre .\n",
      "trg = i went to europe before the war .\n",
      "predicted trg = i went to europe before the war . <eos>\n",
      "****************************************\n",
      "src =  son fils fut tué en duel .\n",
      "trg = his son had been killed in a duel .\n",
      "predicted trg = his sons were killed in a <unk> . <eos>\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "for example_idx in range(40,50):\n",
    "\n",
    "    src = vars(train_data.examples[example_idx])['SRC']\n",
    "    trg = vars(train_data.examples[example_idx])['TRG']\n",
    "\n",
    "    print(\"src = \",' '.join(src))\n",
    "    print(\"trg =\" ,' '.join(trg))\n",
    "\n",
    "    translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
    "    print(\"predicted trg =\" ,' '.join(translation))\n",
    "    print(\"*\"*40)\n",
    "    #print(f'predicted trg = {' '.join(translation)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
